{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census Data Decision Tree using ID3 algroithm and Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#labeledPositive = number of positive data points, totalSize = total number of instances in data set\n",
    "#note this function will only work for data sets with a binary class label (true/false, >=50k/<50k, etc)\n",
    "#also \"positive\" and \"negative\" are interchangable - entropy will calc the same for either.\n",
    "#Returns entropy value\n",
    "def calc_entropy(labeledPositive,totalSize):\n",
    "    labeledNegative = totalSize-labeledPositive\n",
    "    #if one of our variables is 0, entropy is 0. Special case to avoid divide by zero errors\n",
    "    if labeledNegative == 0 or labeledPositive == 0:\n",
    "        return 0\n",
    "    #else return entropy calculation\n",
    "    else:\n",
    "        return -((labeledPositive/totalSize)*math.log((labeledPositive/totalSize),2)+(labeledNegative/totalSize)*math.log((labeledNegative/totalSize),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns Information Gain value given a list of feature names and a dataframe containing 2 columns - the data column in question and the class label\n",
    "def calc_IG(values,data):\n",
    "    totalIG=0\n",
    "    #find names of feature and class label\n",
    "    names = []\n",
    "    for col in data.columns:\n",
    "        names.append(col)\n",
    "    classLabel = data[names[1]].unique()\n",
    "    #for every unique value in data, add to IG\n",
    "    for v in values:     \n",
    "        #isolate just data with the unique attribute we care about\n",
    "        tempdf = data[data[names[0]] == v]\n",
    "        numRows = len(tempdf.index)\n",
    "        #find all values with a certain class label\n",
    "        numOccur = len(tempdf[tempdf[names[1]] == classLabel[0]])\n",
    "        #add to totalIG using information gain calculation\n",
    "        totalIG += ((numRows/len(data.index))*(calc_entropy(numOccur,numRows)))\n",
    "    numOccur = len(data[data[names[1]] == classLabel[0]])\n",
    "    #finally, add total IG and subtract the calculations we did above in for loop per IG calculation\n",
    "    totalIG = calc_entropy(numOccur,len(data.index)) - totalIG    \n",
    "    return totalIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a string with the name of the feature with highest IG\n",
    "def chooseFeature(data):\n",
    "    #setup class label\n",
    "    classLabel = data[data.columns[-1]]\n",
    "    data = data.drop([data.columns[-1]], axis=1)\n",
    "    bestIG=0\n",
    "    bestIGname=data.columns[0]\n",
    "    \n",
    "    #for each column, calculate IG. If it is more than bestIG, replace bestIG. \n",
    "    for col in data:\n",
    "        #attach one column of data with class label\n",
    "        toCalc = pd.concat([data[col], classLabel], axis=1, sort=False)\n",
    "        #setup column names\n",
    "        name=[]\n",
    "        for c in toCalc.columns:\n",
    "            name.append(c)\n",
    "        #send values to calc_IG function to calculate\n",
    "        tempIG=calc_IG(data[col].unique(),toCalc)\n",
    "        #if better or equal, replace\n",
    "        if tempIG >= bestIG:\n",
    "            bestIG = tempIG\n",
    "            bestIGname = name[0]\n",
    "    return bestIGname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns dictionary of dictionaries given data and a list of attributes to test\n",
    "def build_tree(data,attr,fulldata):\n",
    "    #setup tree, header names, and class label for use later\n",
    "    tree = {}\n",
    "    name=[]\n",
    "    for c in data.columns:\n",
    "        name.append(c)\n",
    "    classLabel = data[data.columns[-1]]\n",
    "    classLabelName = name[-1]\n",
    "    del name[-1]\n",
    "    \n",
    "    if (data.empty):\n",
    "        #exit case dataset is empty, we are done\n",
    "        return (fulldata[fulldata.columns[-1]].value_counts()[:1].index[0])\n",
    "        \n",
    "        \n",
    "    elif (len(classLabel.unique())==1):\n",
    "        #exit case for all examples positive/negative\n",
    "        \n",
    "        #return the value of class label \n",
    "        return classLabel.unique().tolist()[0]    \n",
    "    \n",
    "    elif len(data.columns.values)==1:\n",
    "        #exit case for tested all attributes\n",
    "        return data.iloc[0][0]\n",
    "    \n",
    "    else:\n",
    "        #else, continue recursion\n",
    "        #select best feature and remove it from list of attributes\n",
    "        featureName = chooseFeature(data)\n",
    "        #get all possible values of target feature\n",
    "        target_vals = fulldata[featureName].unique()\n",
    "        #the output of this for loop will be put into a new dictionary, which will become the \"value\" to featureName in tree\n",
    "        temptree = {}\n",
    "        #recurse for each unique value of selected feature with only those rows included\n",
    "        for value in target_vals:\n",
    "            #select only rows with each value of target feature\n",
    "            tempdata = data.loc[data[featureName] == value]\n",
    "            #drop featureName column data as we have already used it\n",
    "            tempdata = tempdata.drop([featureName],axis=1)\n",
    "            #set   value : {recursively build new tree}\n",
    "            temptree[value] = build_tree(tempdata,attr,fulldata)\n",
    "        \n",
    "        #add to tree\n",
    "        tree[featureName] = temptree\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates and prints accuracy of test data given data and a tree (dictionary of dictionaries)\n",
    "def predict(testData,tree):\n",
    "    #setup testData to be processed\n",
    "    name=[]\n",
    "    for c in testData.columns:\n",
    "        name.append(c)\n",
    "    classLabel = testData[testData.columns[-1]]\n",
    "    classLabelName = name[-1]\n",
    "    del name[-1]\n",
    "    \n",
    "    prediction=[]\n",
    "    #for each row in testData:\n",
    "    for i in range(len(testData)):\n",
    "        #add prediction for row i to list\n",
    "        guess=predict_row(testData.loc[i],tree)\n",
    "        #had 1 or 2 instances where guess was None out of the dataset of 15000, not sure why. \n",
    "        #this if statement simply adds an incorrect guess if somehow predict_row returns none.\n",
    "        if guess is None:\n",
    "            guess=\"N/A\"\n",
    "        #had some errors with list/non-list values - if guess is returned in list form, append its value to prediction\n",
    "        if type(guess) is list:\n",
    "            prediction.append(guess[0])\n",
    "        #else, simply append guess\n",
    "        else:\n",
    "            prediction.append(guess)\n",
    "    \n",
    "    numCorrect=0\n",
    "    #count correct. This could easily be combined into one for loop but I seperated them for clarity.    \n",
    "    for i in range(len(testData)):\n",
    "        if (prediction[i] == classLabel[i]):\n",
    "            numCorrect = numCorrect+1\n",
    "    \n",
    "    #print(\">50K:\",prediction.count('>50K'),\"\\n<=50K:\",prediction.count('<=50K'))\n",
    "    #print information\n",
    "    print(\"********************\\n\\n\")\n",
    "    print(\"Number of testing examples:\",len(testData))\n",
    "    print(\"Number correctly predicted:\",numCorrect)\n",
    "    print(\"Number incorrectly predicted:\",(len(testData)-numCorrect))\n",
    "    print(\"Accuracy:\",(numCorrect/len(testData))*100,\"%\")\n",
    "    print(\"\\n\\n********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns string with prediction for a given row based on tree\n",
    "def predict_row(row,tree):\n",
    "    for key in tree:\n",
    "        #Find the inner dictionary that matches with the key that the tree is looking for. In the inner dictionary,\n",
    "        #set newTree to be the dictionary or value corresponding to the value that 'row' has.\n",
    "        #this line is very confusing, I tried to explain it as clearly as possible  \n",
    "        newTree = tree[key][row[key]]\n",
    "        #if value is also a dictionary, recurse\n",
    "        if isinstance(newTree,dict): \n",
    "            return predict_row(row,newTree)\n",
    "        #if newtree's NOT a dictionary: return value\n",
    "        else:\n",
    "            return newTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "\n",
      "\n",
      "Number of testing examples: 15028\n",
      "Number correctly predicted: 11759\n",
      "Number incorrectly predicted: 3269\n",
      "Accuracy: 78.24727175938249 %\n",
      "\n",
      "\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"assets/census_training.csv\")\n",
    "dfTest = pd.read_csv(\"assets/census_training_test.csv\")\n",
    "\n",
    "#for testing purposes\n",
    "#df = pd.read_csv(\"assets/playtennis.csv\")\n",
    "#dfTest = pd.read_csv(\"assets/playtennis.csv\")\n",
    "\n",
    "#setup for build_tree function\n",
    "name = []\n",
    "for c in df.columns:\n",
    "    name.append(c)\n",
    "del name[-1]\n",
    "\n",
    "\n",
    "\n",
    "#run prediction\n",
    "predict(dfTest,build_tree(df,name,df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
